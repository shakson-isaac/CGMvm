# HEAPâ€‘CGM GPUÂ Conda Environment (`mambatf`)

This README captures **everything required to reproduce or update** the GPUâ€‘ready environment you just finished configuring on the GoogleÂ Cloud VM.

---

## 1Â Â Snapshot the environment now

```bash
# inside the VM (anywhere)
conda activate mambatf
conda env export --no-builds > environment.yml  # specification without local build hashes
```

Commit or copy `environment.yml` to your project repo / bucket.  This file plus the steps below is enough to rebuild the environment from scratch.

---

## 2Â Â Reâ€‘create the VMâ€‘level NVIDIA stack (only when provisioning a fresh VM)

1. **Create/attach a GPU instance** (TeslaÂ T4) in ComputeÂ Engine.
2. SSH in and install the driver (already done, but for a fresh VM):

   ```bash
   sudo apt update && sudo apt install -y cuda-drivers
   ```
3. **Install CUDAÂ ToolkitÂ 12.3** (for `nvcc`) and expose it:

   ```bash
   # add NVIDIA repo once; then
   sudo apt install -y cuda-toolkit-12-3
   echo 'export PATH=/usr/local/cuda-12.3/bin:$PATH' >> ~/.bashrc
   echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
   source ~/.bashrc
   nvcc --version   # sanityâ€‘check
   ```

---

## 3Â Â Create the Conda env from the saved spec

```bash
conda env create -f environment.yml   # or YOUR/LOCATION/environment.yml
conda activate mambatf
```

If you ever need to rebuild from an **empty** YAML (no local hashes) use:

```bash
conda env remove -n mambatf -y        # wipe
conda env create -f environment.yml   # recreate
```

---

## 4Â Â Postâ€‘create steps (wheels)

The YAML purposely omits two binary wheels that cannot be resolved by Conda:

| Package                             | Wheel (PyÂ 3.10)                                             | Install cmd                                                              |
| ----------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------------ |
| `pytorch-forecasting`               | current main branch                                         | `pip install git+https://github.com/sktime/pytorch-forecasting.git@main` |
| `mamba-ssmÂ 2.2.6` (TorchÂ 2.4 wheel) | `mamba_ssmâ€‘2.2.6+cu11torch2.4â€‘cp310â€‘cp310â€‘linux_x86_64.whl` | `pip install ./mamba_ssmâ€‘2.2.6+cu11torch2.4*.whl`                        |

Copy the wheel into the VM first (or `wget` from the release page) then run the install commands **inside** the active environment.

---

## 5Â Â Smokeâ€‘test script

Run the snippet below to confirm the GPU and all three libraries are operational:

```python
import torch, pytorch_forecasting, mamba_ssm
print(torch.__version__, torch.version.cuda, torch.cuda.get_device_name(0))
print('forecasting', pytorch_forecasting.__version__)
print('mambaâ€‘ssm ', mamba_ssm.__version__)
print('CUDA OK ?', torch.cuda.is_available())
```

Expected: GPU name (TeslaÂ T4) appears and no ImportErrors.

---

## 6Â Â Updating the environment

* **Add a package**:Â  `conda install PKG` *or* `pip install PKG`, then `conda env export --no-builds > environment.yml` to refresh the spec.
* **Upgrade PyTorch**: bump both `pytorch` and matching `pytorchâ€‘cuda` in YAML, then rebuild or `conda env update -f environment.yml`.

---

## 7Â Â Troubleshooting checklist

1. `nvcc: command not found` â€“> ensure CUDA Toolkit path is exported in `.bashrc`.
2. `torch.cuda.is_available() == False` â€“> driver not installed or mismatched; rerun `nvidia-smi` to debug.
3. `pip install mamba-ssm` tries to compile â€“> use preâ€‘built wheel or install Toolkit.

Happy modeling! Â ğŸ‰
